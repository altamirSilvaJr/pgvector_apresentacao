{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary nltk unidecode gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGVECTOR\n",
    "Extensão para o PostreSQL que permite manipular vetores de alta dimensionalidade diretamente no banco de dados.\n",
    "Principais caso de uso:\n",
    "- Recuperação de informação (busca por similaridade entre vetores. Sistema de recomendação, pesquisa semântica, classificação de documentos);\n",
    "- Análise de texto (análise de sentimento, agrupamento de documentos, detecção de spam);\n",
    "- Processamento de linguagem natural (PLN/NLP);\n",
    "- Análise de imagens  (buscar características de imagens em formato vetorial)\n",
    "\n",
    "## Embeddings\n",
    "Representação numérica de um objeto (palavra, frase, imagem, entidade) em espaço vetorial. Embeddings são projetados de forma que objetos semelhantes estejam próximos uns dos outros.\n",
    "\n",
    "## Tokenização\n",
    "Processo que envolve dividir um texto em unidades menores (tokens) que podem ser palavras, subpalavras ou caracteres.\n",
    "\n",
    "## Word2Vec\n",
    "Técnica de aprendizado de representação de palavras que mapeia palavras do vocavulário para vetores numéricos, permitindo a captura de semelhanças semânticas e relações entre palavras.\n",
    "\n",
    "## Dockerfile\n",
    "```\n",
    "FROM postgres:latest\n",
    "\n",
    "RUN apt-get update && apt-get install -y postgresql-16-pgvector\n",
    "```\n",
    "\n",
    "## docker-compose.yml\n",
    "```\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  db:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: pgvector-container\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    environment:\n",
    "      POSTGRES_PASSWORD: password\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_DB: mydatabase\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    \"João gostava de estudar Ciência de Dados.\",\n",
    "    \"Pedro gosta de comer chocolate.\",\n",
    "    \"O cãozinho de Maria é muito bagunceiro.\",\n",
    "    \"Jonas detesta chocolate amargo.\",\n",
    "    \"Eles voltaram muito tarde da festa de ontem.\",\n",
    "    \"As ondas do mar estavam agitadas no último final de semana.\",\n",
    "    \"O pôr do sol estava magnífico naquele dia.\",\n",
    "    \"Amanhã será um dia ensolarado.\",\n",
    "    \"O gato de João fugiu de casa na semana passada.\",\n",
    "    \"Joaquina gosta de João.\",\n",
    "    \"Pedro detesta Maria.\",\n",
    "    \"Maria estuda no colégio mais caro da cidade.\",\n",
    "    \"Maria, Pedro e João estudam na mesma escola.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('rslp') # conjunto de dados com modelos pré-treinados em pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['João', 'gostava', 'de', 'estudar', 'Ciência', 'de', 'Dados', '.'],\n",
       " ['Pedro', 'gosta', 'de', 'comer', 'chocolate', '.'],\n",
       " ['O', 'cãozinho', 'de', 'Maria', 'é', 'muito', 'bagunceiro', '.'],\n",
       " ['Jonas', 'detesta', 'chocolate', 'amargo', '.'],\n",
       " ['Eles', 'voltaram', 'muito', 'tarde', 'da', 'festa', 'de', 'ontem', '.'],\n",
       " ['As',\n",
       "  'ondas',\n",
       "  'do',\n",
       "  'mar',\n",
       "  'estavam',\n",
       "  'agitadas',\n",
       "  'no',\n",
       "  'último',\n",
       "  'final',\n",
       "  'de',\n",
       "  'semana',\n",
       "  '.'],\n",
       " ['O', 'pôr', 'do', 'sol', 'estava', 'magnífico', 'naquele', 'dia', '.'],\n",
       " ['Amanhã', 'será', 'um', 'dia', 'ensolarado', '.'],\n",
       " ['O',\n",
       "  'gato',\n",
       "  'de',\n",
       "  'João',\n",
       "  'fugiu',\n",
       "  'de',\n",
       "  'casa',\n",
       "  'na',\n",
       "  'semana',\n",
       "  'passada',\n",
       "  '.'],\n",
       " ['Joaquina', 'gosta', 'de', 'João', '.'],\n",
       " ['Pedro', 'detesta', 'Maria', '.'],\n",
       " ['Maria', 'estuda', 'no', 'colégio', 'mais', 'caro', 'da', 'cidade', '.'],\n",
       " ['Maria', ',', 'Pedro', 'e', 'João', 'estudam', 'na', 'mesma', 'escola', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [word_tokenize(frase) for frase in frases]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetores = {}\n",
    "for palavra in modelo.wv.key_to_index:\n",
    "    vetores[palavra] = modelo.wv[palavra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['.', 'de', 'João', 'Maria', 'Pedro', 'O', 'na', 'dia', 'muito', 'do', 'semana', 'chocolate', 'da', 'gosta', 'no', 'detesta', 'Eles', 'festa', 'ontem', 'tarde', 'voltaram', 'As', 'é', 'amargo', 'Jonas', 'bagunceiro', 'cãozinho', 'comer', 'Dados', 'Ciência', 'estudar', 'gostava', 'ondas', 'escola', 'mar', 'mesma', 'estudam', 'e', ',', 'cidade', 'caro', 'mais', 'colégio', 'estuda', 'Joaquina', 'passada', 'casa', 'fugiu', 'gato', 'ensolarado', 'um', 'será', 'Amanhã', 'naquele', 'magnífico', 'estava', 'sol', 'pôr', 'final', 'último', 'agitadas', 'estavam'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetores.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processar o texto?\n",
    "Em alguns casos pode ser interessante processar o texto para facilitar o alcance do objetivo do modelo que será treinado. Essa processamento pode incluir a remoção de caracteres especiais (como acentos e pontuação), a retirada de algumas palavras que podem ser pouco interessante para a propsota do modelo e até mesmo a remoção de prefixos e sufixos de palavras, deixando apenas o radical da palavra. Ainda existem casos de remoção de palavras inteiras (stop words).\n",
    "\n",
    "### Vantagens de processar o texto\n",
    "- Reduz a variabilidade;\n",
    "- Simplifica o vocabulário;\n",
    "- Melhora a qualidade dos vetores;\n",
    "- Facilita a comparação de palavras.\n",
    "\n",
    "\n",
    "### Quando processar?\n",
    "- Análise de sentimentos;\n",
    "- Classificação de texto.\n",
    "\n",
    "### Quando não processar?\n",
    "- Análise de texto multilíngue;\n",
    "- Preservação de informações contextuais.\n",
    "\n",
    "### Stop Words\n",
    "Palavras comuns geralmente são removidas por serem consideradas menos imortantes para a compreensão do significado do texto. É interessante removê-las ao treinar um modelo Word2Vect quando elas não contribuem significativamente para a representação semântica das frases e podem introduzir ruídos  nos vetores. Removê-las ajuda a melhorar a qualidade dos vetores gerados pelo modelo, dando foco nas palavras mais informativas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "tabela_pontuacao = str.maketrans('', '', string.punctuation) # tabela de tradução para remover pontuação\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joao gostava de estudar ciencia de dados',\n",
       " 'pedro gosta de comer chocolate',\n",
       " 'o caozinho de maria e muito bagunceiro',\n",
       " 'jonas detesta chocolate amargo',\n",
       " 'eles voltaram muito tarde da festa de ontem',\n",
       " 'as ondas do mar estavam agitadas no ultimo final de semana',\n",
       " 'o por do sol estava magnifico naquele dia',\n",
       " 'amanha sera um dia ensolarado',\n",
       " 'o gato de joao fugiu de casa na semana passada',\n",
       " 'joaquina gosta de joao',\n",
       " 'pedro detesta maria',\n",
       " 'maria estuda no colegio mais caro da cidade',\n",
       " 'maria pedro e joao estudam na mesma escola']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_processadas = []\n",
    "for frase in frases:\n",
    "    nova_frase = unidecode(frase.lower())\n",
    "    nova_frase = nova_frase.translate(tabela_pontuacao)\n",
    "    frases_processadas.append(nova_frase)\n",
    "frases_processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['joao', 'gostava', 'estudar', 'ciencia', 'dados'],\n",
       " ['pedro', 'gosta', 'comer', 'chocolate'],\n",
       " ['caozinho', 'maria', 'bagunceiro'],\n",
       " ['jonas', 'detesta', 'chocolate', 'amargo'],\n",
       " ['voltaram', 'tarde', 'festa', 'ontem'],\n",
       " ['ondas', 'mar', 'agitadas', 'ultimo', 'final', 'semana'],\n",
       " ['sol', 'magnifico', 'naquele', 'dia'],\n",
       " ['amanha', 'sera', 'dia', 'ensolarado'],\n",
       " ['gato', 'joao', 'fugiu', 'casa', 'semana', 'passada'],\n",
       " ['joaquina', 'gosta', 'joao'],\n",
       " ['pedro', 'detesta', 'maria'],\n",
       " ['maria', 'estuda', 'colegio', 'caro', 'cidade'],\n",
       " ['maria', 'pedro', 'joao', 'estudam', 'mesma', 'escola']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_sem_stopwords = []\n",
    "for frase in frases_processadas:\n",
    "    tokens = word_tokenize(frase)\n",
    "    tokens_sem_stopwords = [token for token in tokens if token not in stop_words]\n",
    "    frases_sem_stopwords.append(tokens_sem_stopwords)\n",
    "frases_sem_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_stopwords = Word2Vec(frases_sem_stopwords, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetores_pgvector = {}\n",
    "for palavra in model_no_stopwords.wv.key_to_index:\n",
    "    vetores_pgvector[palavra] = model_no_stopwords.wv[palavra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.4647954e-04,  2.2584017e-04,  5.1015122e-03,  9.0103038e-03,\n",
       "       -9.3133766e-03, -7.1234256e-03,  6.4616804e-03,  8.9799482e-03,\n",
       "       -5.0202948e-03, -3.7688590e-03,  7.3812045e-03, -1.5383899e-03,\n",
       "       -4.5441505e-03,  6.5576700e-03, -4.8700930e-03, -1.8123193e-03,\n",
       "        2.8764834e-03,  9.9321571e-04, -8.2821585e-03, -9.4561912e-03,\n",
       "        7.3133963e-03,  5.0778156e-03,  6.7643747e-03,  7.6566049e-04,\n",
       "        6.3580386e-03, -3.4075845e-03, -9.4601139e-04,  5.7735941e-03,\n",
       "       -7.5312802e-03, -3.9393934e-03, -7.5085098e-03, -9.2570693e-04,\n",
       "        9.5463926e-03, -7.3265536e-03, -2.3370429e-03, -1.9368682e-03,\n",
       "        8.0819856e-03, -5.9340512e-03,  4.5197543e-05, -4.7506811e-03,\n",
       "       -9.5978351e-03,  5.0087199e-03, -8.7746382e-03, -4.3968926e-03,\n",
       "       -2.8846580e-05, -2.9816179e-04, -7.6701660e-03,  9.6239913e-03,\n",
       "        4.9895034e-03,  9.2347767e-03, -8.1596468e-03,  4.5028282e-03,\n",
       "       -4.1350583e-03,  8.2927366e-04,  8.5041607e-03, -4.4665299e-03,\n",
       "        4.5185336e-03, -6.7984601e-03, -3.5486983e-03,  9.4000688e-03,\n",
       "       -1.5764814e-03,  3.2357415e-04, -4.1348180e-03, -7.6865559e-03,\n",
       "       -1.5106944e-03,  2.4689082e-03, -8.9483947e-04,  5.5325376e-03,\n",
       "       -2.7427245e-03,  2.2547625e-03,  5.4617762e-03,  8.3502010e-03,\n",
       "       -1.4510771e-03, -9.2153344e-03,  4.3727043e-03,  5.7129416e-04,\n",
       "        7.4472935e-03, -8.2048547e-04, -2.6376876e-03, -8.7542851e-03,\n",
       "       -8.5708918e-04,  2.8203195e-03,  5.4056626e-03,  7.0589338e-03,\n",
       "       -5.7098512e-03,  1.8539020e-03,  6.0931635e-03, -4.7911792e-03,\n",
       "       -3.1085634e-03,  6.8039228e-03,  1.6309317e-03,  1.9914885e-04,\n",
       "        3.4774416e-03,  2.1857559e-04,  9.6308989e-03,  5.0663068e-03,\n",
       "       -8.9214966e-03, -7.0427358e-03,  9.0863236e-04,  6.3943421e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetores_pgvector['joao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão ao banco de dados PostgreSQL bem-sucedida!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mydatabase\",\n",
    "        user=\"postgres\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"Conexão ao banco de dados PostgreSQL bem-sucedida!\")\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Erro ao conectar ao banco de dados PostgreSQL:\", e)\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "                CREATE EXTENSION IF NOT EXISTS vector\n",
    "               \"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS palavras;\n",
    "               \n",
    "    CREATE TABLE IF NOT EXISTS palavras (\n",
    "        palavra VARCHAR PRIMARY KEY,\n",
    "        vetor VECTOR\n",
    "    );     \n",
    "\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for palavra, vetor in vetores_pgvector.items():\n",
    "    vetor_lista = vetor.tolist()\n",
    "    cursor.execute(\"\"\"\n",
    "                    INSERT INTO palavras (palavra, vetor) VALUES (%s, %s)\n",
    "                   \"\"\", (palavra, vetor_lista))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('joao', '[-0.00054647954,0.00022584017,0.005101512,0.009010304,-0.009313377,-0.0071234256,0.0064616804,0.008979948,-0.005020295,-0.003768859,0.0073812045,-0.00153839,-0.0045441505,0.00655767,-0.004870093,-0.0018123193,0.0028764834,0.0009932157,-0.0082821585,-0.009456191,0.0073133963,0.0050778156,0.0067643747,0.0007656605,0.0063580386,-0.0034075845,-0.0009460114,0.005773594,-0.00753128,-0.0039393934,-0.00750851,-0.00092570693,0.009546393,-0.0073265536,-0.002337043,-0.0019368682,0.008081986,-0.0059340512,4.5197543e-05,-0.004750681,-0.009597835,0.00500872,-0.008774638,-0.0043968926,-2.884658e-05,-0.0002981618,-0.007670166,0.009623991,0.0049895034,0.009234777,-0.008159647,0.0045028282,-0.0041350583,0.00082927366,0.008504161,-0.00446653,0.0045185336,-0.00679846,-0.0035486983,0.009400069,-0.0015764814,0.00032357415,-0.004134818,-0.007686556,-0.0015106944,0.0024689082,-0.00089483947,0.0055325376,-0.0027427245,0.0022547625,0.005461776,0.008350201,-0.001451077,-0.009215334,0.0043727043,0.00057129416,0.0074472935,-0.00082048547,-0.0026376876,-0.008754285,-0.0008570892,0.0028203195,0.0054056626,0.007058934,-0.005709851,0.001853902,0.0060931635,-0.004791179,-0.0031085634,0.006803923,0.0016309317,0.00019914885,0.0034774416,0.00021857559,0.009630899,0.005066307,-0.008921497,-0.007042736,0.00090863236,0.006394342]')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM palavras WHERE palavra = 'joao'\")\n",
    "print(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS frases;\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS frases (\n",
    "        id INT PRIMARY KEY,\n",
    "        frase_original TEXT,\n",
    "        frase_processada TEXT,\n",
    "        vetor_medio VECTOR\n",
    "    )               \n",
    "\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calcular_vetor_medio(vetores):\n",
    "    if not vetores:\n",
    "        return None\n",
    "    return np.mean(vetores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetores_frases = []\n",
    "for frase_sem_stopwords in frases_sem_stopwords:\n",
    "    vetores = [vetores_pgvector[palavra] for palavra in frase_sem_stopwords if palavra in vetores_pgvector]\n",
    "    vetor_medio = calcular_vetor_medio(vetores)\n",
    "    vetores_frases.append(vetor_medio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueViolation",
     "evalue": "duplicate key value violates unique constraint \"frases_pkey\"\nDETAIL:  Key (id)=(1) already exists.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vetor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vetores_frases):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m                   INSERT INTO frases (id, frase_original, frase_processada, vetor_medio)\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m                   VALUES (\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m                   \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrases\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrases_sem_stopwords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvetor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"frases_pkey\"\nDETAIL:  Key (id)=(1) already exists.\n"
     ]
    }
   ],
   "source": [
    "for i, vetor in enumerate(vetores_frases):\n",
    "    cursor.execute(\"\"\"\n",
    "                   INSERT INTO frases (id, frase_original, frase_processada, vetor_medio)\n",
    "                   VALUES (%s, %s, %s, %s)\n",
    "                   \"\"\",\n",
    "                   (i+1, frases[i], ' '.join(frases_sem_stopwords[i]), vetor.tolist())\n",
    "                   )\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'João gostava de estudar Ciência de Dados.', 'joao gostava estudar ciencia dados', '[-0.0009843915,0.0013499345,0.0042601265,0.007225538,-0.00032018227,0.0016333204,0.0012480567,0.0024502166,-0.0006330492,0.0012726279,0.0028621575,-0.0023449624,0.0024246182,0.0031382677,-0.0015405394,-0.0007161265,-0.0022034966,-0.00043933978,-0.005264936,-0.0034959875,0.0028117823,0.003761574,0.0016754474,0.0018044317,0.0021358952,0.0018088408,0.0004844314,-0.0024331056,-0.0022926782,-0.0031391673,0.0013739725,-0.0049681445,0.0046518464,-0.0021005461,0.00010945108,0.00010853154,0.002217096,-0.00016748719,-0.00092074525,-0.005265587,-0.00277698,7.727938e-05,-0.0043182364,0.0014041269,-0.0008110654,-0.003093484,-0.004139456,3.9967803e-05,0.0006629106,0.0027177634,-0.00014669579,-0.00068213156,-0.0025521684,-2.9078034e-05,0.00073985336,-0.001147366,0.0010252175,-0.0038997785,-0.00396998,-0.0017264324,-0.0013694796,0.0030565336,0.0011366396,-0.0009721086,-0.004217553,0.0028539593,-0.0014628444,0.0040539196,-0.0008010232,0.0016249653,0.002405893,0.004827268,-0.00030309203,-0.0016556215,0.000989463,-0.0021836462,0.002529711,-0.0010285713,-0.0007063883,-0.0013479929,-0.0027754498,-0.0035625312,0.0014618391,-0.00038645358,-0.004247335,0.0028482652,0.001062124,-0.00438334,-0.003939576,0.0039114617,0.0013402401,0.00038406407,0.0031423774,0.002307554,0.002869246,0.0032739579,-0.0037041232,0.0009760307,-0.0032406352,0.001215697]')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT * FROM frases WHERE id = 1\")\n",
    "print(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_frase(frase):\n",
    "    tabela_pontuacao = str.maketrans('', '', string.punctuation)\n",
    "    frase = frase.translate(tabela_pontuacao)\n",
    "    frase = unidecode(frase.lower())\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = word_tokenize(frase.lower())\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberto\n",
      "nao\n",
      "gosta\n",
      "chocolate\n"
     ]
    }
   ],
   "source": [
    "frase_teste = 'Roberto não gosta de chocolate.'\n",
    "frase_teste_tokens = processar_frase(frase_teste)\n",
    "\n",
    "vetores_teste = []\n",
    "for palavra in frase_teste_tokens:\n",
    "    print(palavra)\n",
    "    cursor.execute(\"SELECT vetor FROM palavras WHERE palavra = %s\", (palavra,))\n",
    "    vetor = cursor.fetchone()\n",
    "    if vetor:\n",
    "        vetores_teste.append(np.array([float(v) for v in vetor[0].replace('[', '').replace(']', '').split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_medio_teste = calcular_vetor_medio([vetor_convertido.astype(np.float64) for vetor_convertido in vetores_teste])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00049554, -0.0016098 , -0.00412288, -0.00062056,  0.00176351,\n",
       "        0.00349005,  0.00365598,  0.00104184, -0.00367784,  0.00285731,\n",
       "       -0.00020399,  0.00308042, -0.00447335,  0.00568538, -0.00495076,\n",
       "       -0.00254299,  0.00303313,  0.00620306,  0.00365064, -0.00692893,\n",
       "        0.00096124, -0.00539215,  0.00859062,  0.00523334, -0.0006254 ,\n",
       "        0.00160205, -0.00054334,  0.00023905, -0.00418783, -0.0007158 ,\n",
       "        0.00673559,  0.00558637,  0.00022624, -0.00317325,  0.00118783,\n",
       "       -0.00228381, -0.00495899, -0.00260047, -0.00333571, -0.00377296,\n",
       "        0.00452925, -0.00630017, -0.00072551,  0.00089764,  0.0048105 ,\n",
       "        0.00138103, -0.00219191, -0.0048679 ,  0.00021265, -0.00490126,\n",
       "        0.00187753, -0.00224078,  0.0006703 , -0.00083671, -0.00479682,\n",
       "       -0.00819418, -0.00601812, -0.00885705, -0.00404016, -0.00745154,\n",
       "        0.0022743 , -0.00536371,  0.00457326,  0.00113259, -0.00336765,\n",
       "        0.00256435,  0.00198295,  0.00224493, -0.00583357,  0.00237225,\n",
       "       -0.00372387,  0.0062095 , -0.00293341,  0.00417592,  0.00496127,\n",
       "       -0.00028266,  0.0010968 ,  0.00744952,  0.00231671, -0.00146157,\n",
       "       -0.00094081, -0.00704096, -0.00192023, -0.00012694, -0.00427727,\n",
       "       -0.00709663,  0.00890475, -0.00156904, -0.00794174, -0.00245519,\n",
       "        0.00021794, -0.00213341,  0.00087286, -0.0023268 ,  0.0009677 ,\n",
       "       -0.00360904,  0.00720223, -0.00045975, -0.00043612,  0.0042918 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetor_medio_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distância Euclidiana '<->'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.027310343541644203, 'Pedro gosta de comer chocolate.'),\n",
       " (0.03802360422236187, 'Jonas detesta chocolate amargo.'),\n",
       " (0.04023367119608727, 'Joaquina gosta de João.'),\n",
       " (0.04313617836985864, 'Amanhã será um dia ensolarado.'),\n",
       " (0.04464102367697765, 'Maria, Pedro e João estudam na mesma escola.'),\n",
       " (0.045788086980364524, 'João gostava de estudar Ciência de Dados.'),\n",
       " (0.047535127376598096,\n",
       "  'As ondas do mar estavam agitadas no último final de semana.'),\n",
       " (0.04826739208787742, 'O gato de João fugiu de casa na semana passada.'),\n",
       " (0.048889674494748805, 'Maria estuda no colégio mais caro da cidade.'),\n",
       " (0.05073106101214586, 'Pedro detesta Maria.'),\n",
       " (0.05095684430171058, 'O pôr do sol estava magnífico naquele dia.'),\n",
       " (0.05261732949249774, 'Eles voltaram muito tarde da festa de ontem.'),\n",
       " (0.056060921840691585, 'O cãozinho de Maria é muito bagunceiro.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT (vetor_medio <-> %s) AS distancia, frase_original FROM frases ORDER BY distancia;', (str(vetor_medio_teste.tolist()),))\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizinho mais próximo '<->' (No WHERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pedro gosta de comer chocolate.',),\n",
       " ('Jonas detesta chocolate amargo.',),\n",
       " ('Joaquina gosta de João.',),\n",
       " ('Amanhã será um dia ensolarado.',),\n",
       " ('Maria, Pedro e João estudam na mesma escola.',),\n",
       " ('João gostava de estudar Ciência de Dados.',),\n",
       " ('As ondas do mar estavam agitadas no último final de semana.',),\n",
       " ('O gato de João fugiu de casa na semana passada.',),\n",
       " ('Maria estuda no colégio mais caro da cidade.',),\n",
       " ('Pedro detesta Maria.',),\n",
       " ('O pôr do sol estava magnífico naquele dia.',),\n",
       " ('Eles voltaram muito tarde da festa de ontem.',),\n",
       " ('O cãozinho de Maria é muito bagunceiro.',)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT frase_original FROM frases ORDER BY vetor_medio <-> %s;', (str(vetor_medio_teste.tolist()),))\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade de coesseno '1 - (coluna_vetor <#> vetor_para_comparar)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7425666315285069, 'Pedro gosta de comer chocolate.'),\n",
       " (0.4231896731969361, 'Jonas detesta chocolate amargo.'),\n",
       " (0.3891526566604284, 'Joaquina gosta de João.'),\n",
       " (0.2454602554859613, 'Amanhã será um dia ensolarado.'),\n",
       " (0.11682969887498029, 'João gostava de estudar Ciência de Dados.'),\n",
       " (0.09552242662101251, 'Maria, Pedro e João estudam na mesma escola.'),\n",
       " (0.07929196358805435, 'Pedro detesta Maria.'),\n",
       " (0.006435425510865844, 'O gato de João fugiu de casa na semana passada.'),\n",
       " (0.0012604440712027376, 'Maria estuda no colégio mais caro da cidade.'),\n",
       " (-0.025241578625914807,\n",
       "  'As ondas do mar estavam agitadas no último final de semana.'),\n",
       " (-0.028630789981419547, 'O cãozinho de Maria é muito bagunceiro.'),\n",
       " (-0.08609496757887203, 'O pôr do sol estava magnífico naquele dia.'),\n",
       " (-0.10649323757373264, 'Eles voltaram muito tarde da festa de ontem.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT 1 - (vetor_medio <=> %s) AS similaridade_cosseno, frase_original FROM frases ORDER BY similaridade_cosseno DESC;', (str(vetor_medio_teste.tolist()),))\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2574333684714931, 'Pedro gosta de comer chocolate.'),\n",
       " (0.5768103268030639, 'Jonas detesta chocolate amargo.'),\n",
       " (0.6108473433395716, 'Joaquina gosta de João.'),\n",
       " (0.7545397445140387, 'Amanhã será um dia ensolarado.'),\n",
       " (0.8831703011250197, 'João gostava de estudar Ciência de Dados.'),\n",
       " (0.9044775733789875, 'Maria, Pedro e João estudam na mesma escola.'),\n",
       " (0.9207080364119457, 'Pedro detesta Maria.'),\n",
       " (0.9935645744891342, 'O gato de João fugiu de casa na semana passada.'),\n",
       " (0.9987395559287973, 'Maria estuda no colégio mais caro da cidade.'),\n",
       " (1.0252415786259148,\n",
       "  'As ondas do mar estavam agitadas no último final de semana.'),\n",
       " (1.0286307899814195, 'O cãozinho de Maria é muito bagunceiro.'),\n",
       " (1.086094967578872, 'O pôr do sol estava magnífico naquele dia.'),\n",
       " (1.1064932375737326, 'Eles voltaram muito tarde da festa de ontem.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT (vetor_medio <=> %s) AS distancia_cosseno, frase_original FROM frases ORDER BY distancia_cosseno;', (str(vetor_medio_teste.tolist()),))\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2574333684714931, 'Pedro gosta de comer chocolate.'),\n",
       " (0.5768103268030639, 'Jonas detesta chocolate amargo.'),\n",
       " (0.6108473433395716, 'Joaquina gosta de João.'),\n",
       " (0.7545397445140387, 'Amanhã será um dia ensolarado.'),\n",
       " (0.8831703011250197, 'João gostava de estudar Ciência de Dados.'),\n",
       " (0.9044775733789875, 'Maria, Pedro e João estudam na mesma escola.'),\n",
       " (0.9207080364119457, 'Pedro detesta Maria.'),\n",
       " (0.9935645744891342, 'O gato de João fugiu de casa na semana passada.'),\n",
       " (0.9987395559287973, 'Maria estuda no colégio mais caro da cidade.'),\n",
       " (1.0252415786259148,\n",
       "  'As ondas do mar estavam agitadas no último final de semana.'),\n",
       " (1.0286307899814195, 'O cãozinho de Maria é muito bagunceiro.'),\n",
       " (1.086094967578872, 'O pôr do sol estava magnífico naquele dia.'),\n",
       " (1.1064932375737326, 'Eles voltaram muito tarde da festa de ontem.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT cosine_distance(vetor_medio, %s) AS distancia_cosseno, frase_original FROM frases ORDER BY distancia_cosseno;', (str(vetor_medio_teste.tolist()),))\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outras Funções\n",
    "l1_distance(vetor, vetor) -> distância do taxi: normalmente usada para calcular distâncias entre dois poontos em um mapa de ruas.\n",
    "\n",
    "l2_distance(vetor, vetor) -> distância euclidiana: geralmente utilizada em reconhecimento de padrões (identificação de objetos em imagens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
